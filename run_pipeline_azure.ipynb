{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486755488
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Install required Python packages\n",
        "%pip install -Uqq azure-ai-ml azure-identity azure-keyvault-secrets azureml-sdk requests pyyaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486755559
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Directory where you want to clone the repository\n",
        "repo_dir = 'gretel-mlops'\n",
        "\n",
        "# Check if the directory exists\n",
        "if not os.path.exists(repo_dir):\n",
        "    # Directory does not exist, clone the repository\n",
        "    !git clone https://github.com/gretelai/gretel-mlops.git\n",
        "else:\n",
        "    print(f\"The directory '{repo_dir}' already exists.\")\n",
        "\n",
        "# Import Gretel MLOps modules\n",
        "gretel_mlops_path = os.getcwd() + \"/gretel-mlops/src/\"\n",
        "if gretel_mlops_path not in sys.path:\n",
        "    sys.path.append(gretel_mlops_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486757446
        }
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from azure.ai.ml import MLClient, dsl, Input, Output\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.keyvault.secrets import SecretClient\n",
        "from azureml.core import Workspace\n",
        "from azure.ai.ml.entities import Environment\n",
        "import yaml\n",
        "import json\n",
        "from gretel_mlops.azure.azureai import pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Connect to Azure ML Workspace and create ML client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486771148
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Load the workspace configuration from the default configuration file\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Create an ML client using the workspace details\n",
        "ml_client = pipeline.create_ml_client(ws.subscription_id, ws.resource_group, ws.name)\n",
        "\n",
        "# Set a name for the environment to be used in the pipeline\n",
        "pipeline_job_env_name = \"gretel-mlops-pipeline\"\n",
        "\n",
        "# Define the path to the YAML configuration file\n",
        "requirements_file = f'{gretel_mlops_path}/gretel_mlops/azure/azureai/requirements.yaml'\n",
        "\n",
        "# Create an Azure ML environment object\n",
        "# This environment specifies the dependencies and runtime context for the pipeline steps\n",
        "# It includes a reference to a Conda environment file and a base Docker image\n",
        "pipeline_job_env = Environment(\n",
        "    name=pipeline_job_env_name,\n",
        "    description=\"Environment for Gretel MLOps pipeline\",\n",
        "    conda_file=requirements_file,  # Path to Conda dependencies file\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest\",  # Base Docker image\n",
        ")\n",
        "\n",
        "# Register or update the environment in the Azure ML workspace\n",
        "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
        "\n",
        "# Print out the details of the registered environment\n",
        "print(\n",
        "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Fetch and load Gretel MLOps configuration from a YAML file\n",
        "\n",
        "The below code create or update a data asset from the config file in the Azure ML workspace.\n",
        "It creates a datastore and will prompt for Azure Storage account details.\n",
        "\n",
        "Note: Azure Storage account name and key required for datastore creation. These can be found in the Azure Portal under your storage account's \"Access keys\" section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486774314
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define the URL of the YAML configuration file\n",
        "config_file = f\"{gretel_mlops_path}/gretel_mlops/configs/config_stroke.yaml\"\n",
        "\n",
        "# Parse the YAML content of the response into a Python dictionary\n",
        "with open(config_file, \"r\") as file:\n",
        "    config_dict = yaml.safe_load(file)\n",
        "\n",
        "# Generate a unique name for the configuration asset based on the dataset name\n",
        "config_asset_name = f\"pipeline-config-{config_dict['dataset']['name']}\"\n",
        "\n",
        "# Call the function to create or update the asset in Azure ML workspace\n",
        "config_asset = pipeline.create_asset_from_config(\n",
        "    ml_client, ws, config_dict, config_asset_name\n",
        ")\n",
        "\n",
        "print(f\"Config asset path: {config_asset.path}\")\n",
        "\n",
        "print(\"Config file:\")\n",
        "yaml.dump(config_dict, sys.stdout, default_flow_style=False, sort_keys=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Build the pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486789791
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define the local src dir to pipeline components\n",
        "src_dir = \"gretel-mlops/src/gretel_mlops/azure/azureai/components/\"\n",
        "\n",
        "# Define the components that will be used in the pipeline.\n",
        "components = pipeline.define_pipeline_components(\n",
        "    subscription_id=ws.subscription_id, \n",
        "    resource_group=ws.resource_group, \n",
        "    workspace_name=ws.name,\n",
        "    pipeline_job_env_name=pipeline_job_env_name, \n",
        "    pipeline_job_env_version=pipeline_job_env.version,\n",
        "    src_dir=src_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486789873
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define the Gretel pipeline\n",
        "@dsl.pipeline(\n",
        "    compute=\"serverless\",\n",
        "    description=\"Gretel MLOps pipeline\",\n",
        ")\n",
        "def gretel_pipeline(pipeline_job_config, gretel_api_key, pipeline_job_registered_model_name):\n",
        "    \"\"\"\n",
        "    Define and configure the Gretel pipeline.\n",
        "\n",
        "    This function constructs a pipeline using predefined components, sets up the necessary configurations, \n",
        "    and links these components together.\n",
        "\n",
        "    Args:\n",
        "        pipeline_job_config (str): Configuration for the pipeline job, defining the execution parameters and settings.\n",
        "        gretel_api_key (str): The API key for authenticating with the Gretel services.\n",
        "        pipeline_job_registered_model_name (str): The registered name of the model used in the pipeline.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the output directories for each step of the pipeline, facilitating access to results and logs.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Define the preprocessing step of the pipeline\n",
        "    preprocess_op = components[\"preprocess\"](config=pipeline_job_config)\n",
        "\n",
        "    # Define the Gretel synthetic data generation step\n",
        "    gretel_op = components[\"gretel\"](\n",
        "        input_dir=preprocess_op.outputs.output_dir,\n",
        "        gretel_api_key=gretel_api_key,\n",
        "        config=pipeline_job_config\n",
        "    )\n",
        "\n",
        "    # Define the training step of the pipeline\n",
        "    train_op = components[\"train\"](\n",
        "        input_dir=preprocess_op.outputs.output_dir, \n",
        "        gretel_dir=gretel_op.outputs.output_dir,\n",
        "        config=pipeline_job_config\n",
        "    )\n",
        "\n",
        "    # Define the evaluation step of the pipeline\n",
        "    eval_op = components[\"evaluate\"](\n",
        "        input_dir=preprocess_op.outputs.output_dir, \n",
        "        model_dir=train_op.outputs.output_dir,\n",
        "        config=pipeline_job_config\n",
        "    )\n",
        "\n",
        "    # Define the model registration step of the pipeline\n",
        "    register_op = components[\"register\"](\n",
        "        eval_dir=eval_op.outputs.output_dir, \n",
        "        model_dir=train_op.outputs.output_dir, \n",
        "        model_display_name=pipeline_job_registered_model_name,\n",
        "        config=pipeline_job_config\n",
        "    )\n",
        "\n",
        "    # Return a dictionary mapping the names of the pipeline steps to their respective output directories\n",
        "    # These outputs can be used as inputs to subsequent steps or for analysis after the pipeline completes\n",
        "    return {\n",
        "        \"pipeline_job_preprocess_outputs\": preprocess_op.outputs.output_dir,\n",
        "        \"pipeline_job_gretel_outputs\": gretel_op.outputs.output_dir,\n",
        "        \"pipeline_job_train_outputs\": train_op.outputs.output_dir,\n",
        "        \"pipeline_job_eval_outputs\": eval_op.outputs.output_dir,\n",
        "        \"pipeline_job_register_outputs\": register_op.outputs.output_dir,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Submit the pipeline job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486789989
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Retrieve Gretel API Key from the Azure Key Vault and secret name where the Gretel API Key is stored\n",
        "gretel_key_vault_name = \"GretelVault\"\n",
        "gretel_secret_name = \"GretelApiKey\"\n",
        "gretel_api_key = pipeline.get_secret(gretel_secret_name, gretel_key_vault_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486610832
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Construct a unique name for the model based on the dataset name from the config\n",
        "registered_model_name = f\"gretel-model-{config_dict['dataset']['name']}\"\n",
        "\n",
        "# Construct a unique name for the experiment under which this pipeline run will be recorded\n",
        "experiment_name = f\"gretel-model-{config_dict['dataset']['name']}-new2\"\n",
        "\n",
        "# Create and submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    job=gretel_pipeline(\n",
        "        pipeline_job_config=Input(type=\"uri_file\", path=config_asset.path),\n",
        "        gretel_api_key=gretel_api_key,\n",
        "        pipeline_job_registered_model_name=registered_model_name\n",
        "    ),\n",
        "    experiment_name=experiment_name,\n",
        ")\n",
        "\n",
        "# Stream the logs of the pipeline job\n",
        "ml_client.jobs.stream(pipeline_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5. Inspect Evaluation Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1715486611755
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Download the output of the pipeline job\n",
        "\n",
        "eval_outputs = \"pipeline_job_eval_outputs\"\n",
        "\n",
        "ml_client.jobs.download(\n",
        "    name=pipeline_job.name,\n",
        "    download_path=\"./\",\n",
        "    output_name=eval_outputs\n",
        ")\n",
        "\n",
        "evaluation_report_file = f\"named-outputs/{eval_outputs}/evaluation.json\"\n",
        "\n",
        "with open(evaluation_report_file, \"r\") as json_file:\n",
        "    evaluation_report = json.load(json_file)\n",
        "\n",
        "# Print the JSON data nicely with indentation\n",
        "print(json.dumps(evaluation_report, indent=4))"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
